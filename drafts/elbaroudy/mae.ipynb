{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9f0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141158a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaskedAutoencoderViT\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "\n",
    "from data import DummyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e233ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(img_size=(65, 650), \\\n",
    "                                       patch_size=(5, 10), \\\n",
    "                                       in_chans=1)\n",
    "\n",
    "dataset = DummyDataset()\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40c689fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# dataset = DummyDataset()\n",
    "# # batch_size = 1\n",
    "# dataloader = DataLoader(dataset)\n",
    "\n",
    "# for batch in dataloader: \n",
    "#     inputs, targets = batch \n",
    "#     print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7dd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/ --port 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df590370",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "logger = pl.loggers.WandbLogger(project=\"\", name=\"\", \n",
    "                                save_dir=\"/data0/practical-sose23/brain-age\", log_model=False)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"validation loss\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")\n",
    "\n",
    "trainer = pl.Trainer(callbacks=[early_stop_callback], overfit_batches=1.0, max_epochs=200, accelerator=\"gpu\", logger=logger)\n",
    "trainer.fit(model, dataloader_train, dataloader_val)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainer = pl.Trainer(limit_train_batches=100, max_epochs=100, devices=1)\n",
    "\n",
    "trainer.fit(model=MaskedAutoencoderViT(img_size=(65, 650), \\\n",
    "                                       patch_size=(5, 10), \\\n",
    "                                       in_chans=1), \\\n",
    "            train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import misc as misc\n",
    "import lr_sched as lr_sched\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler,\n",
    "                    log_writer=None,\n",
    "                    args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
    "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
    "        if (data_iter_step + 1) % accum_iter == 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb18d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(img_size=(65, 650), \\\n",
    "                                       patch_size=(5, 10), \\\n",
    "                                       in_chans=1)\n",
    "\n",
    "dataset = DummyDataset()\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac491985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir: log\n",
      "torch.Size([1, 1, 65, 650])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m args \u001b[38;5;241m=\u001b[39m myargs()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(args.accum_iter)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_one_epoch(model, dataloader, optimizer, device, \u001b[38;5;241m1\u001b[39m, loss_scaler, log_writer, args)\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device, epoch, loss_scaler, log_writer, args)\u001b[0m\n\u001b[1;32m     28\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 31\u001b[0m     loss, _, _ \u001b[38;5;241m=\u001b[39m model(samples, mask_ratio\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmask_ratio)\n\u001b[1;32m     33\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math\u001b[38;5;241m.\u001b[39misfinite(loss_value):\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data0/practical-sose23/brain-age/drafts/elbaroudy/model.py:349\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward\u001b[0;34m(self, imgs, mask_ratio)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs, mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m):\n\u001b[0;32m--> 349\u001b[0m     latent, mask, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_encoder(imgs, mask_ratio)\n\u001b[1;32m    350\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_decoder(latent, ids_restore)  \u001b[38;5;66;03m# [N, L, p*p*3]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_loss(imgs, pred, mask)\n",
      "File \u001b[0;32m/data0/practical-sose23/brain-age/drafts/elbaroudy/model.py:266\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward_encoder\u001b[0;34m(self, x, mask_ratio)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask_ratio):\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#         x = torch.stack(x)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# embed patches\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#         print(type(x.cpu()))\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 266\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# add pos embed w/o cls token\u001b[39;00m\n\u001b[1;32m    269\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed[:, \u001b[38;5;241m1\u001b[39m:, :]\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/timm/models/vision_transformer.py:165\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# FIXME look at relaxing size constraints\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 165\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "from misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "loss_scaler = NativeScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_writer = SummaryWriter(log_dir='log')\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.95))\n",
    "class myargs(): \n",
    "    def __init__(self): \n",
    "        self.mask_ratio = 0.75\n",
    "        self.accum_iter = 1 \n",
    "        self.warmup_epochs = 40\n",
    "        self.lr = 0.01\n",
    "args = myargs()\n",
    "# print(args.accum_iter)\n",
    "train_one_epoch(model, dataloader, optimizer, device, 1, loss_scaler, log_writer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Current device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25882969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x = model.pos_embed\n",
    "# print(x.squeeze(0).shape)\n",
    "# attn = torch.matmul(x.squeeze(0), x.squeeze(0).T)\n",
    "# # attn[torch.eye(attn.shape[0], dtype=bool)] = 0\n",
    "\n",
    "# # plt.plot(attn[0][torch.argsort(attn[0])])\n",
    "# plt.imshow(attn)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca02ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = model.pos_embed\n",
    "# for block in model.blocks:\n",
    "#     x = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ef306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(dummy_EEG_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6971f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_count = torch.cuda.device_count()\n",
    "# print(device_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe6520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
