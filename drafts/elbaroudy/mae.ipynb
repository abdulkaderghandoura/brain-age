{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9f0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "141158a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MaskedAutoencoderViT\n",
    "from torch.utils.data import DataLoader\n",
    "import torch \n",
    "import lightning.pytorch as pl\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "!export CUDA_VISIBLE_DEVICES=0\n",
    "import wandb\n",
    "from data import DummyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4667ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DummyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e233ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(img_size=(65, 4000), \\\n",
    "                                       patch_size=(5, 10), \\\n",
    "                                       in_chans=1)\n",
    "\n",
    "dataset = DummyDataset()\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40c689fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 65, 4000])\n"
     ]
    }
   ],
   "source": [
    "# dataset = DummyDataset()\n",
    "# # batch_size = 1\n",
    "# dataloader = DataLoader(dataset)\n",
    "\n",
    "# for batch in dataset: \n",
    "#     inputs, targets = batch \n",
    "#     print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb7dd99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reload_ext tensorboard\n",
    "# %tensorboard --logdir=lightning_logs/ --port 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df590370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msama-elbaroudy\u001b[0m (\u001b[33mbrain-age\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/u/home/elbs/brain-age/drafts/elbaroudy/wandb/run-20230521_111040-h2wkdt80</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/brain-age/brain-age/runs/h2wkdt80' target=\"_blank\">mae_overfitting_EEG_sample_AdamW_optim</a></strong> to <a href='https://wandb.ai/brain-age/brain-age' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/brain-age/brain-age' target=\"_blank\">https://wandb.ai/brain-age/brain-age</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/brain-age/brain-age/runs/h2wkdt80' target=\"_blank\">https://wandb.ai/brain-age/brain-age/runs/h2wkdt80</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/home/elbs/.conda/envs/brain-age/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /u/home/elbs/.conda/envs/brain-age/lib/python3.11/si ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "/u/home/elbs/.conda/envs/brain-age/lib/python3.11/site-packages/lightning/fabric/plugins/environments/slurm.py:165: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /u/home/elbs/.conda/envs/brain-age/lib/python3.11/si ...\n",
      "  rank_zero_warn(\n",
      "You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name           | Type       | Params\n",
      "----------------------------------------------\n",
      "0 | patch_embed    | PatchEmbed | 52.2 K\n",
      "1 | blocks         | ModuleList | 302 M \n",
      "2 | norm           | LayerNorm  | 2.0 K \n",
      "3 | decoder_embed  | Linear     | 524 K \n",
      "4 | decoder_blocks | ModuleList | 25.2 M\n",
      "5 | decoder_norm   | LayerNorm  | 1.0 K \n",
      "6 | decoder_pred   | Linear     | 25.7 K\n",
      "----------------------------------------------\n",
      "328 M     Trainable params\n",
      "8.0 M     Non-trainable params\n",
      "336 M     Total params\n",
      "1,344.498 Total estimated model params size (MB)\n",
      "/u/home/elbs/.conda/envs/brain-age/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 256 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/u/home/elbs/.conda/envs/brain-age/lib/python3.11/site-packages/lightning/pytorch/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "028abc3c7dbb436aa405b763a09bf451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>loss_epoch</td><td>█▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss_step</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>99</td></tr><tr><td>loss_epoch</td><td>0.02745</td></tr><tr><td>loss_step</td><td>0.02745</td></tr><tr><td>trainer/global_step</td><td>99</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">mae_overfitting_EEG_sample_AdamW_optim</strong> at: <a href='https://wandb.ai/brain-age/brain-age/runs/h2wkdt80' target=\"_blank\">https://wandb.ai/brain-age/brain-age/runs/h2wkdt80</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230521_111040-h2wkdt80/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login()\n",
    "logger = pl.loggers.WandbLogger(project=\"brain-age\", name=\"mae_overfitting_EEG_sample_AdamW_optim\", \n",
    "                                save_dir=\"/u/home/elbs/brain-age/drafts/elbaroudy\", log_model=False)\n",
    "\n",
    "# early_stop_callback = EarlyStopping(monitor=\"validation loss\", min_delta=0.00, patience=3, verbose=False, mode=\"max\")\n",
    "\n",
    "trainer = pl.Trainer(overfit_batches=1.0, max_epochs=100, accelerator=\"gpu\", logger=logger, devices=1)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)\n",
    "# trainer.fit(model, dataloader_train, dataloader_val)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69bf6611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaskedAutoencoderViT(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 1024, kernel_size=(5, 10), stride=(5, 10))\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-23): 24 x Block(\n",
      "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder_embed): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (decoder_blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (decoder_pred): Linear(in_features=512, out_features=50, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b0b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# trainer = pl.Trainer(limit_train_batches=100, max_epochs=100, devices=1)\n",
    "\n",
    "# trainer.fit(model=MaskedAutoencoderViT(img_size=(65, 650), \\\n",
    "#                                        patch_size=(5, 10), \\\n",
    "#                                        in_chans=1), \\\n",
    "#             train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "import misc as misc\n",
    "import lr_sched as lr_sched\n",
    "def train_one_epoch(model: torch.nn.Module,\n",
    "                    data_loader: Iterable, optimizer: torch.optim.Optimizer,\n",
    "                    device: torch.device, epoch: int, loss_scaler,\n",
    "                    log_writer=None,\n",
    "                    args=None):\n",
    "    model.train(True)\n",
    "    metric_logger = misc.MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 20\n",
    "\n",
    "    accum_iter = args.accum_iter\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if log_writer is not None:\n",
    "        print('log_dir: {}'.format(log_writer.log_dir))\n",
    "\n",
    "    for data_iter_step, (samples, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "\n",
    "        # we use a per iteration (instead of per epoch) lr scheduler\n",
    "        if data_iter_step % accum_iter == 0:\n",
    "            lr_sched.adjust_learning_rate(optimizer, data_iter_step / len(data_loader) + epoch, args)\n",
    "\n",
    "        samples = samples.to(device, non_blocking=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss, _, _ = model(samples, mask_ratio=args.mask_ratio)\n",
    "\n",
    "        loss_value = loss.item()\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(\"Loss is {}, stopping training\".format(loss_value))\n",
    "            sys.exit(1)\n",
    "\n",
    "        loss /= accum_iter\n",
    "        loss_scaler(loss, optimizer, parameters=model.parameters(),\n",
    "                    update_grad=(data_iter_step + 1) % accum_iter == 0)\n",
    "        if (data_iter_step + 1) % accum_iter == 0:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "        metric_logger.update(loss=loss_value)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        metric_logger.update(lr=lr)\n",
    "\n",
    "        loss_value_reduce = misc.all_reduce_mean(loss_value)\n",
    "        if log_writer is not None and (data_iter_step + 1) % accum_iter == 0:\n",
    "            \"\"\" We use epoch_1000x as the x-axis in tensorboard.\n",
    "            This calibrates different curves when batch size changes.\n",
    "            \"\"\"\n",
    "            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)\n",
    "            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)\n",
    "            log_writer.add_scalar('lr', lr, epoch_1000x)\n",
    "\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb18d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskedAutoencoderViT(img_size=(65, 650), \\\n",
    "                                       patch_size=(5, 10), \\\n",
    "                                       in_chans=1)\n",
    "\n",
    "dataset = DummyDataset()\n",
    "batch_size = 1\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac491985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_dir: log\n",
      "torch.Size([1, 1, 65, 650])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (c10::Half) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m args \u001b[38;5;241m=\u001b[39m myargs()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(args.accum_iter)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m train_one_epoch(model, dataloader, optimizer, device, \u001b[38;5;241m1\u001b[39m, loss_scaler, log_writer, args)\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, data_loader, optimizer, device, epoch, loss_scaler, log_writer, args)\u001b[0m\n\u001b[1;32m     28\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 31\u001b[0m     loss, _, _ \u001b[38;5;241m=\u001b[39m model(samples, mask_ratio\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmask_ratio)\n\u001b[1;32m     33\u001b[0m loss_value \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math\u001b[38;5;241m.\u001b[39misfinite(loss_value):\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/data0/practical-sose23/brain-age/drafts/elbaroudy/model.py:349\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward\u001b[0;34m(self, imgs, mask_ratio)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs, mask_ratio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m):\n\u001b[0;32m--> 349\u001b[0m     latent, mask, ids_restore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_encoder(imgs, mask_ratio)\n\u001b[1;32m    350\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_decoder(latent, ids_restore)  \u001b[38;5;66;03m# [N, L, p*p*3]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_loss(imgs, pred, mask)\n",
      "File \u001b[0;32m/data0/practical-sose23/brain-age/drafts/elbaroudy/model.py:266\u001b[0m, in \u001b[0;36mMaskedAutoencoderViT.forward_encoder\u001b[0;34m(self, x, mask_ratio)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_encoder\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask_ratio):\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m#         x = torch.stack(x)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;66;03m# embed patches\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m#         print(type(x.cpu()))\u001b[39;00m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 266\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embed(x)\n\u001b[1;32m    268\u001b[0m         \u001b[38;5;66;03m# add pos embed w/o cls token\u001b[39;00m\n\u001b[1;32m    269\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed[:, \u001b[38;5;241m1\u001b[39m:, :]\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/timm/models/vision_transformer.py:165\u001b[0m, in \u001b[0;36mPatchEmbed.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# FIXME look at relaxing size constraints\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m H \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m W \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m], \\\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mW\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 165\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(x)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/.conda/envs/brain-age/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    460\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (c10::Half) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "from misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "loss_scaler = NativeScaler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "log_writer = SummaryWriter(log_dir='log')\n",
    "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.95))\n",
    "class myargs(): \n",
    "    def __init__(self): \n",
    "        self.mask_ratio = 0.75\n",
    "        self.accum_iter = 1 \n",
    "        self.warmup_epochs = 40\n",
    "        self.lr = 0.01\n",
    "args = myargs()\n",
    "# print(args.accum_iter)\n",
    "train_one_epoch(model, dataloader, optimizer, device, 1, loss_scaler, log_writer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Current device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25882969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# x = model.pos_embed\n",
    "# print(x.squeeze(0).shape)\n",
    "# attn = torch.matmul(x.squeeze(0), x.squeeze(0).T)\n",
    "# # attn[torch.eye(attn.shape[0], dtype=bool)] = 0\n",
    "\n",
    "# # plt.plot(attn[0][torch.argsort(attn[0])])\n",
    "# plt.imshow(attn)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca02ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = model.pos_embed\n",
    "# for block in model.blocks:\n",
    "#     x = block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810ef306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(dummy_EEG_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6971f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_count = torch.cuda.device_count()\n",
    "# print(device_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abe6520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
